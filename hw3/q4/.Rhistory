# Plotting out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
## CV minimum deviance selection
b.min = coef(cvl, select="min")
# value of lamda:
log(cvl$lambda.min)
sum(b.min!=0) # this gives the coefficent not 0
##########
# Predict number of shares
lhat_shares = predict(cvl, x) # log value of shares
hat_shares = exp(lhat_shares) # predicted values of shares
head (hat_shares, 50)
# Changing predicted number of shares into viral prediction(t_viral)
threshold_viral = ifelse(hat_shares > 1400, 1, 0)
head(threshold_viral, 50)
# Creating new variable "viral"
viral = ifelse(online_news$shares > 1400, 1, 0)
head(viral, 20)
# Creating confusion matrix
confusion_1= table(y = viral, yhat = threshold_viral)
print(confusion_1)
sum(diag(confusion_1))/sum(confusion_1) # This gives the sample accuracy for model 1
##### Model 2
# Running logistic lasso regression and cross validate with viral as the dependent variable
# family = "binomial" in this code is used to do a logistic regression instead of normal regression
#(verb just prints progress)
viral_cvl = cv.gamlr(x, viral, nfold=10, family="binomial", verb=TRUE)
# Plotting  the out-of-sample deviance as a function of log lambda
plot(viral_cvl, bty="n")
## CV minimum deviance selection
b.min = coef(viral_cvl, select="min")
print(b.min)
log(viral_cvl$lambda.min)
sum(b.min!=0) # This is random because of the CV randomness.
# Predicting number of viral
hat_viral = predict(viral_cvl, x, select = "min")
hat_viral[which.min(hat_viral)]
head (hat_viral, 50)
# Changing hat_viral to true/false prediction
b_hat_viral = ifelse(hat_viral > 0, 1, 0)
head(b_hat_viral, 50)
# Creating confusion matirx
confusion_2= table(y = viral, yhat = b_hat_viral)
print(confusion_2)
sum(diag(confusion_2))/sum(confusion_2) # This is the sample accuracy of model 2
##### Comparison of models
table(viral) # The actual number of viral or not viral articles
20082/39644  # 50.66 percent of articles were not viral which is the null hypothesis
print(confusion_1)
sum(diag(confusion_1))/sum(confusion_1) # The sample accuracy for model 1 is 56.8 percent
# Hence model 1 is (56.8-50.66) about a 6 percent improvement to the null model
print(confusion_2)
sum(diag(confusion_2))/sum(confusion_2) # The sample accuracy of model 2 is 63 percent
# Hence model 2 is 12.5 percent improvement to null model and about 6.2 percent improvement to model 1
# In conclusion based on True Positive Rate, False Positve Rate, False Discovery Rate, and general acuracy Model 2 does better than Model 1.
##### Importiing, viewing, and analizing data
online_news <- read.csv("~/GitHub/SDS323_Spring2020/hw2/q3/online_news.csv")
#View(online_news)
str(online_news)
##### Regress first and threshold second
hist(online_news$shares)
# We should apply the log transformation since shares is very skewed
# After log transformation
hist(log(online_news$shares))
# Fitting lasso regression and doing cross validation of K=10 folds to automate finding independent variables and training and testing my data multiple times simultaneously
library(gamlr)
# Creating a matrix of all the independent varaibles exculuding url from online_news data using the sparse.model.matrix function
x = sparse.model.matrix(log(shares) ~ . - url, data=online_news)[,-1] # -1 drops intercept
y = log(online_news$shares) # Pulling out `y' for convenience and taking the log of the dependent variable shares
# Fiting lasso regression to the data and doing cross validation of k=10 folds using the cv.gamlr command which does both tasks
# Verb = TRUE prints progress
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# Plotting out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
## CV minimum deviance selection
b.min = coef(cvl, select="min")
# value of lamda:
log(cvl$lambda.min)
sum(b.min!=0) # this gives the coefficent not 0
##########
# Predict number of shares
lhat_shares = predict(cvl, x) # log value of shares
hat_shares = exp(lhat_shares) # predicted values of shares
head (hat_shares, 50)
# Changing predicted number of shares into viral prediction(t_viral)
threshold_viral = ifelse(hat_shares > 1400, 1, 0)
head(threshold_viral, 50)
# Creating new variable "viral"
viral = ifelse(online_news$shares > 1400, 1, 0)
head(viral, 20)
# Creating confusion matrix
confusion_1= table(y = viral, yhat = threshold_viral)
print(confusion_1)
sum(diag(confusion_1))/sum(confusion_1) # This gives the sample accuracy for model 1
##### Model 2
# Running logistic lasso regression and cross validate with viral as the dependent variable
# family = "binomial" in this code is used to do a logistic regression instead of normal regression
#(verb just prints progress)
viral_cvl = cv.gamlr(x, viral, nfold=10, family="binomial", verb=TRUE)
# Plotting  the out-of-sample deviance as a function of log lambda
plot(viral_cvl, bty="n")
## CV minimum deviance selection
b.min = coef(viral_cvl, select="min")
print(b.min)
log(viral_cvl$lambda.min)
sum(b.min!=0) # This is random because of the CV randomness.
# Predicting number of viral
hat_viral = predict(viral_cvl, x, select = "min")
plot(hat_viral)
##### Importiing, viewing, and analizing data
online_news <- read.csv("~/GitHub/SDS323_Spring2020/hw2/q3/online_news.csv")
#View(online_news)
str(online_news)
##### Regress first and threshold second
hist(online_news$shares)
# We should apply the log transformation since shares is very skewed
# After log transformation
hist(log(online_news$shares))
# Fitting lasso regression and doing cross validation of K=10 folds to automate finding independent variables and training and testing my data multiple times simultaneously
library(gamlr)
# Creating a matrix of all the independent varaibles exculuding url from online_news data using the sparse.model.matrix function
x = sparse.model.matrix(log(shares) ~ . - url, data=online_news)[,-1] # -1 drops intercept
y = log(online_news$shares) # Pulling out `y' for convenience and taking the log of the dependent variable shares
# Fiting lasso regression to the data and doing cross validation of k=10 folds using the cv.gamlr command which does both tasks
# Verb = TRUE prints progress
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# Plotting out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
## CV minimum deviance selection
b.min = coef(cvl, select="min")
# value of lamda:
log(cvl$lambda.min)
sum(b.min!=0) # this gives the coefficent not 0
##########
# Predict number of shares
lhat_shares = predict(cvl, x) # log value of shares
hat_shares = exp(lhat_shares) # predicted values of shares
head (hat_shares, 50)
# Changing predicted number of shares into viral prediction(t_viral)
threshold_viral = ifelse(hat_shares > 1400, 1, 0)
head(threshold_viral, 50)
# Creating new variable "viral"
viral = ifelse(online_news$shares > 1400, 1, 0)
head(viral, 20)
# Creating confusion matrix
confusion_1= table(y = viral, yhat = threshold_viral)
print(confusion_1)
sum(diag(confusion_1))/sum(confusion_1) # This gives the sample accuracy for model 1
##### Model 2
# Running logistic lasso regression and cross validate with viral as the dependent variable
# family = "binomial" in this code is used to do a logistic regression instead of normal regression
#(verb just prints progress)
viral_cvl = cv.gamlr(x, viral, nfold=10, family="binomial", verb=TRUE)
# Plotting  the out-of-sample deviance as a function of log lambda
plot(viral_cvl, bty="n")
## CV minimum deviance selection
b.min = coef(viral_cvl, select="min")
print(b.min)
log(viral_cvl$lambda.min)
sum(b.min!=0) # This is random because of the CV randomness.
# Predicting number of viral
hat_viral = predict(viral_cvl, x, select = "min")
plot(hat_viral)
head (hat_viral, 50)
# Changing hat_viral to true/false prediction
b_hat_viral = ifelse(hat_viral > 3, 1, 0)
head(b_hat_viral, 50)
# Creating confusion matirx
confusion_2= table(y = viral, yhat = b_hat_viral)
print(confusion_2)
sum(diag(confusion_2))/sum(confusion_2) # This is the sample accuracy of model 2
##### Comparison of models
table(viral) # The actual number of viral or not viral articles
20082/39644  # 50.66 percent of articles were not viral which is the null hypothesis
print(confusion_1)
sum(diag(confusion_1))/sum(confusion_1) # The sample accuracy for model 1 is 56.8 percent
# Hence model 1 is (56.8-50.66) about a 6 percent improvement to the null model
print(confusion_2)
sum(diag(confusion_2))/sum(confusion_2) # The sample accuracy of model 2 is 63 percent
# Hence model 2 is 12.5 percent improvement to null model and about 6.2 percent improvement to model 1
# In conclusion based on True Positive Rate, False Positve Rate, False Discovery Rate, and general acuracy Model 2 does better than Model 1.
##### Importiing, viewing, and analizing data
online_news <- read.csv("~/GitHub/SDS323_Spring2020/hw2/q3/online_news.csv")
#View(online_news)
str(online_news)
##### Regress first and threshold second
hist(online_news$shares)
# We should apply the log transformation since shares is very skewed
# After log transformation
hist(log(online_news$shares))
# Fitting lasso regression and doing cross validation of K=10 folds to automate finding independent variables and training and testing my data multiple times simultaneously
library(gamlr)
# Creating a matrix of all the independent varaibles exculuding url from online_news data using the sparse.model.matrix function
x = sparse.model.matrix(log(shares) ~ . - url, data=online_news)[,-1] # -1 drops intercept
y = log(online_news$shares) # Pulling out `y' for convenience and taking the log of the dependent variable shares
# Fiting lasso regression to the data and doing cross validation of k=10 folds using the cv.gamlr command which does both tasks
# Verb = TRUE prints progress
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# Plotting out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
## CV minimum deviance selection
b.min = coef(cvl, select="min")
# value of lamda:
log(cvl$lambda.min)
sum(b.min!=0) # this gives the coefficent not 0
##########
# Predict number of shares
lhat_shares = predict(cvl, x) # log value of shares
hat_shares = exp(lhat_shares) # predicted values of shares
head (hat_shares, 50)
# Changing predicted number of shares into viral prediction(t_viral)
threshold_viral = ifelse(hat_shares > 1400, 1, 0)
head(threshold_viral, 50)
# Creating new variable "viral"
viral = ifelse(online_news$shares > 1400, 1, 0)
head(viral, 20)
# Creating confusion matrix
confusion_1= table(y = viral, yhat = threshold_viral)
print(confusion_1)
sum(diag(confusion_1))/sum(confusion_1) # This gives the sample accuracy for model 1
##### Model 2
# Running logistic lasso regression and cross validate with viral as the dependent variable
# family = "binomial" in this code is used to do a logistic regression instead of normal regression
#(verb just prints progress)
viral_cvl = cv.gamlr(x, viral, nfold=10, family="binomial", verb=TRUE)
# Plotting  the out-of-sample deviance as a function of log lambda
plot(viral_cvl, bty="n")
## CV minimum deviance selection
b.min = coef(viral_cvl, select="min")
print(b.min)
log(viral_cvl$lambda.min)
sum(b.min!=0) # This is random because of the CV randomness.
# Predicting number of viral
hat_viral = predict(viral_cvl, x, select = "min", type="response")
plot(hat_viral)
head (hat_viral, 50)
# Changing hat_viral to true/false prediction
b_hat_viral = ifelse(hat_viral > 0, 1, 0)
head(b_hat_viral, 50)
# Creating confusion matirx
confusion_2= table(y = viral, yhat = b_hat_viral)
print(confusion_2)
sum(diag(confusion_2))/sum(confusion_2) # This is the sample accuracy of model 2
##### Comparison of models
table(viral) # The actual number of viral or not viral articles
20082/39644  # 50.66 percent of articles were not viral which is the null hypothesis
print(confusion_1)
sum(diag(confusion_1))/sum(confusion_1) # The sample accuracy for model 1 is 56.8 percent
# Hence model 1 is (56.8-50.66) about a 6 percent improvement to the null model
print(confusion_2)
sum(diag(confusion_2))/sum(confusion_2) # The sample accuracy of model 2 is 63 percent
# Hence model 2 is 12.5 percent improvement to null model and about 6.2 percent improvement to model 1
# In conclusion based on True Positive Rate, False Positve Rate, False Discovery Rate, and general acuracy Model 2 does better than Model 1.
##### Importiing, viewing, and analizing data
online_news <- read.csv("~/GitHub/SDS323_Spring2020/hw2/q3/online_news.csv")
#View(online_news)
str(online_news)
##### Regress first and threshold second
hist(online_news$shares)
# We should apply the log transformation since shares is very skewed
# After log transformation
hist(log(online_news$shares))
# Fitting lasso regression and doing cross validation of K=10 folds to automate finding independent variables and training and testing my data multiple times simultaneously
library(gamlr)
# Creating a matrix of all the independent varaibles exculuding url from online_news data using the sparse.model.matrix function
x = sparse.model.matrix(log(shares) ~ . - url, data=online_news)[,-1] # -1 drops intercept
y = log(online_news$shares) # Pulling out `y' for convenience and taking the log of the dependent variable shares
# Fiting lasso regression to the data and doing cross validation of k=10 folds using the cv.gamlr command which does both tasks
# Verb = TRUE prints progress
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# Plotting out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
## CV minimum deviance selection
b.min = coef(cvl, select="min")
# value of lamda:
log(cvl$lambda.min)
sum(b.min!=0) # this gives the coefficent not 0
##########
# Predict number of shares
lhat_shares = predict(cvl, x) # log value of shares
hat_shares = exp(lhat_shares) # predicted values of shares
head (hat_shares, 50)
# Changing predicted number of shares into viral prediction(t_viral)
threshold_viral = ifelse(hat_shares > 1400, 1, 0)
head(threshold_viral, 50)
# Creating new variable "viral"
viral = ifelse(online_news$shares > 1400, 1, 0)
head(viral, 20)
# Creating confusion matrix
confusion_1= table(y = viral, yhat = threshold_viral)
print(confusion_1)
sum(diag(confusion_1))/sum(confusion_1) # This gives the sample accuracy for model 1
##### Model 2
# Running logistic lasso regression and cross validate with viral as the dependent variable
# family = "binomial" in this code is used to do a logistic regression instead of normal regression
#(verb just prints progress)
viral_cvl = cv.gamlr(x, viral, nfold=10, family="binomial", verb=TRUE)
# Plotting  the out-of-sample deviance as a function of log lambda
plot(viral_cvl, bty="n")
## CV minimum deviance selection
b.min = coef(viral_cvl, select="min")
print(b.min)
log(viral_cvl$lambda.min)
sum(b.min!=0) # This is random because of the CV randomness.
# Predicting number of viral
hat_viral = predict(viral_cvl, x, select = "min", type="response")
plot(hat_viral)
head (hat_viral, 50)
# Changing hat_viral to true/false prediction
b_hat_viral = ifelse(hat_viral > 0.5, 1, 0)
head(b_hat_viral, 50)
# Creating confusion matirx
confusion_2= table(y = viral, yhat = b_hat_viral)
print(confusion_2)
sum(diag(confusion_2))/sum(confusion_2) # This is the sample accuracy of model 2
##### Comparison of models
table(viral) # The actual number of viral or not viral articles
20082/39644  # 50.66 percent of articles were not viral which is the null hypothesis
print(confusion_1)
sum(diag(confusion_1))/sum(confusion_1) # The sample accuracy for model 1 is 56.8 percent
# Hence model 1 is (56.8-50.66) about a 6 percent improvement to the null model
print(confusion_2)
sum(diag(confusion_2))/sum(confusion_2) # The sample accuracy of model 2 is 63 percent
# Hence model 2 is 12.5 percent improvement to null model and about 6.2 percent improvement to model 1
# In conclusion based on True Positive Rate, False Positve Rate, False Discovery Rate, and general acuracy Model 2 does better than Model 1.
library(ggplot2)
countdata = read.csv("../data/congress109.csv", header=TRUE, row.names=1)
memberdata = read.csv("../data/congress109members.csv", header=TRUE, row.names=1)
# First normalize phrase counts to phrase frequencies.
# (often a sensible first step for count data, before z-scoring)
Z = countdata/rowSums(countdata)
# PCA
pc2 = prcomp(Z, scale=TRUE, rank=2)
loadings = pc2$rotation
scores = pc2$x
# Question 1: where do the observations land in PC space?
# a biplot shows the first two PCs
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2')
# Confusingly, the default color mapping has Democrats as red and republicans as blue.  This might be confusing, so let's fix that:
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2') + scale_color_manual(values=c("blue", "grey", "red"))
# Interpretation: the first PC axis primarily has Republicans as positive numbers and Democrats as negative numbers
# Question 2: how are the individual PCs loaded on the original variables?
# The top words associated with each component
o1 = order(loadings[,1], decreasing=TRUE)
colnames(Z)[head(o1,25)]
colnames(Z)[tail(o1,25)]
o2 = order(loadings[,2], decreasing=TRUE)
colnames(Z)[head(o2,25)]
colnames(Z)[tail(o2,25)]
library(ggplot2)
countdata = read.csv("../data/congress109.csv", header=TRUE, row.names=1)
memberdata = read.csv("../data/congress109members.csv", header=TRUE, row.names=1)
# First normalize phrase counts to phrase frequencies.
# (often a sensible first step for count data, before z-scoring)
Z = countdata/rowSums(countdata)
# PCA
pc2 = prcomp(Z, scale=TRUE, rank=2)
loadings = pc2$rotation
scores = pc2$x
# Question 1: where do the observations land in PC space?
# a biplot shows the first two PCs
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2')
# Confusingly, the default color mapping has Democrats as red and republicans as blue.  This might be confusing, so let's fix that:
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2') + scale_color_manual(values=c("blue", "grey", "red"))
# Interpretation: the first PC axis primarily has Republicans as positive numbers and Democrats as negative numbers
# Question 2: how are the individual PCs loaded on the original variables?
# The top words associated with each component
o1 = order(loadings[,1], decreasing=TRUE)
colnames(Z)[head(o1,25)]
colnames(Z)[tail(o1,25)]
o2 = order(loadings[,2], decreasing=TRUE)
colnames(Z)[head(o2,25)]
colnames(Z)[tail(o2,25)]
library(readr)
congress109 <- read_csv("Google Drive/UT Austin courses/SDS323/data/congress109.csv")
View(congress109)
library(readr)
congress109members <- read_csv("Google Drive/UT Austin courses/SDS323/data/congress109members.csv")
View(congress109members)
library(ggplot2)
countdata = read.csv("../data/congress109.csv", header=TRUE, row.names=1)
memberdata = read.csv("../data/congress109members.csv", header=TRUE, row.names=1)
# First normalize phrase counts to phrase frequencies.
# (often a sensible first step for count data, before z-scoring)
Z = countdata/rowSums(countdata)
# PCA
pc2 = prcomp(Z, scale=TRUE, rank=2)
loadings = pc2$rotation
scores = pc2$x
# Question 1: where do the observations land in PC space?
# a biplot shows the first two PCs
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2')
# Confusingly, the default color mapping has Democrats as red and republicans as blue.  This might be confusing, so let's fix that:
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2') + scale_color_manual(values=c("blue", "grey", "red"))
# Interpretation: the first PC axis primarily has Republicans as positive numbers and Democrats as negative numbers
# Question 2: how are the individual PCs loaded on the original variables?
# The top words associated with each component
o1 = order(loadings[,1], decreasing=TRUE)
colnames(Z)[head(o1,25)]
colnames(Z)[tail(o1,25)]
o2 = order(loadings[,2], decreasing=TRUE)
colnames(Z)[head(o2,25)]
colnames(Z)[tail(o2,25)]
library(ggplot2)
countdata = read.csv("../data/congress109.csv", header=TRUE, row.names=1)
memberdata = read.csv("../data/congress109members.csv", header=TRUE, row.names=1)
# First normalize phrase counts to phrase frequencies.
# (often a sensible first step for count data, before z-scoring)
Z = countdata/rowSums(countdata)
# PCA
pc2 = prcomp(Z, scale=TRUE, rank=2)
loadings = pc2$rotation
scores = pc2$x
# Question 1: where do the observations land in PC space?
# a biplot shows the first two PCs
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2')
# Confusingly, the default color mapping has Democrats as red and republicans as blue.  This might be confusing, so let's fix that:
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2') + scale_color_manual(values=c("blue", "grey", "red"))
# Interpretation: the first PC axis primarily has Republicans as positive numbers and Democrats as negative numbers
# Question 2: how are the individual PCs loaded on the original variables?
# The top words associated with each component
o1 = order(loadings[,1], decreasing=TRUE)
colnames(Z)[head(o1,25)]
colnames(Z)[tail(o1,25)]
o2 = order(loadings[,2], decreasing=TRUE)
colnames(Z)[head(o2,25)]
colnames(Z)[tail(o2,25)]
# First normalize phrase counts to phrase frequencies.
# (often a sensible first step for count data, before z-scoring)
Z = countdata/rowSums(countdata)
library(ggplot2)
countdata = read.csv("../data/congress109.csv", header=TRUE, row.names=1)
memberdata = read.csv("../data/congress109members.csv", header=TRUE, row.names=1)
# First normalize phrase counts to phrase frequencies.
# (often a sensible first step for count data, before z-scoring)
Z = countdata/rowSums(countdata)
# PCA
pc2 = prcomp(Z, scale=TRUE, rank=2)
loadings = pc2$rotation
scores = pc2$x
# Question 1: where do the observations land in PC space?
# a biplot shows the first two PCs
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2')
# Confusingly, the default color mapping has Democrats as red and republicans as blue.  This might be confusing, so let's fix that:
qplot(scores[,1], scores[,2], color=memberdata$party, xlab='Component 1', ylab='Component 2') + scale_color_manual(values=c("blue", "grey", "red"))
# Interpretation: the first PC axis primarily has Republicans as positive numbers and Democrats as negative numbers
# Question 2: how are the individual PCs loaded on the original variables?
# The top words associated with each component
o1 = order(loadings[,1], decreasing=TRUE)
colnames(Z)[head(o1,25)]
colnames(Z)[tail(o1,25)]
o2 = order(loadings[,2], decreasing=TRUE)
colnames(Z)[head(o2,25)]
colnames(Z)[tail(o2,25)]
load(iris)
head(iris, 3)
data <- load(iris)
head(data, 10)
data <- load(iris)
view(data)
head(iris, 10)
data <- load(iris)
View(data)
head(iris, 10)
data <- load(iris)
summary(data)
head(iris, 10)
iris_data <- load(iris)
summary(iris_data)
head(iris, 10)
load(iris)
summary(iris)
head(iris, 10)
getwd
getwd()
setwd("/Users/dhwanit/Google Drive/UT Austin Courses/SDS323_Spring2020/hw3/s4")
load(iris)
summary(iris)
head(iris, 10)
setwd("/Users/dhwanit/Google Drive/UT Austin Courses/SDS323_Spring2020/hw3/q4")
load(iris)
summary(iris)
head(iris, 10)
getwd()
setwd("/Users/dhwanit/Google Drive/UT Austin Courses/SDS323_Spring2020/hw3/q4")
load(iris)
summary(iris)
head(iris, 10)
# log transform
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA - scale. = TRUE is highly
# advisable, but default is FALSE.
ir.pca <- prcomp(log.ir,
center = TRUE,
scale. = TRUE)
# print method
print(ir.pca)
setwd("/Users/dhwanit/Google Drive/UT Austin Courses/SDS323_Spring2020/hw3/q4")
load(iris)
summary(iris)
head(iris, 10)
# log transform
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA - scale. = TRUE is highly
# advisable, but default is FALSE.
ir.pca <- prcomp(log.ir,
center = TRUE,
scale. = TRUE, rank = 2)
# print method
print(ir.pca)
