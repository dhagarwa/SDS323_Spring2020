library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvr$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
# Apply CV Ridge regression to data
cvr <- cv.glmnet(
x ,
y ,
alpha = 0
)
# plot MSE as a function of log(lambda)
plot(cvr)
min(cvr$cvm)       # minimum MSE
## [1] 0.06679016  #value observed
cvr$lambda.min     # lambda for this min MSE
cvr$cvm[cvr$lambda == cvr$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvr$lambda.1se  # lambda for this MSE
#fitted coefficients at minimum MSE
coef(cvr, select="min")
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
# import data and examine it
greenbuildings <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q1/greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvr$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
# import data and examine it
greenbuildings <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q1/greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvr$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
library(CVR)
# import data and examine it
greenbuildings <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q1/greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvr$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
install.packages(c("backports", "bayestestR", "BH", "blavaan", "broom", "callr", "caret", "caTools", "checkmate", "cli", "countrycode", "crosstalk", "dbplyr", "dendextend", "DescTools", "deSolve", "digest", "doBy", "dplyr", "DT", "effectsize", "emmeans", "fansi", "farver", "fma", "forcats", "foreach", "forecast", "fracdiff", "fs", "fst", "future", "future.apply", "GGally", "ggformula", "ggm", "ggplot2", "ggraph", "ggrepel", "ggridges", "ggstance", "glue", "gplots", "graphlayouts", "gtools", "Hmisc", "hms", "huge", "igraph", "insight", "isoband", "jsonlite", "knitr", "latticeExtra", "lifecycle", "lme4", "lmerTest", "loo", "lubridate", "matrixStats", "mcmc", "MCMCpack", "mime", "mnormt", "ModelMetrics", "modelr", "mosaic", "MuMIn", "mvtnorm", "nFactors", "nloptr", "nonnest2", "parameters", "pbkrtest", "performance", "pillar", "plyr", "prettyunits", "pROC", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantmod", "quantreg", "raster", "Rcpp", "RcppArmadillo", "RcppParallel", "reshape2", "RJSONIO", "rlang", "rstan", "rstanarm", "rstudioapi", "shiny", "shinyjs", "sp", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tibble", "tidyr", "tidyselect", "tinytex", "TSP", "TTR", "vctrs", "xfun", "xml2", "xts", "yaml", "zoo"))
install.packages(c("backports", "bayestestR", "BH", "blavaan", "broom", "callr", "caret", "caTools", "checkmate", "cli", "countrycode", "crosstalk", "dbplyr", "dendextend", "DescTools", "deSolve", "digest", "doBy", "dplyr", "DT", "effectsize", "emmeans", "fansi", "farver", "fma", "forcats", "foreach", "forecast", "fracdiff", "fs", "fst", "future", "future.apply", "GGally", "ggformula", "ggm", "ggplot2", "ggraph", "ggrepel", "ggridges", "ggstance", "glue", "gplots", "graphlayouts", "gtools", "Hmisc", "hms", "huge", "igraph", "insight", "isoband", "jsonlite", "knitr", "latticeExtra", "lifecycle", "lme4", "lmerTest", "loo", "lubridate", "matrixStats", "mcmc", "MCMCpack", "mime", "mnormt", "ModelMetrics", "modelr", "mosaic", "MuMIn", "mvtnorm", "nFactors", "nloptr", "nonnest2", "parameters", "pbkrtest", "performance", "pillar", "plyr", "prettyunits", "pROC", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantmod", "quantreg", "raster", "Rcpp", "RcppArmadillo", "RcppParallel", "reshape2", "RJSONIO", "rlang", "rstan", "rstanarm", "rstudioapi", "shiny", "shinyjs", "sp", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tibble", "tidyr", "tidyselect", "tinytex", "TSP", "TTR", "vctrs", "xfun", "xml2", "xts", "yaml", "zoo"))
install.packages(c("backports", "bayestestR", "BH", "blavaan", "broom", "callr", "caret", "caTools", "checkmate", "cli", "countrycode", "crosstalk", "dbplyr", "dendextend", "DescTools", "deSolve", "digest", "doBy", "dplyr", "DT", "effectsize", "emmeans", "fansi", "farver", "fma", "forcats", "foreach", "forecast", "fracdiff", "fs", "fst", "future", "future.apply", "GGally", "ggformula", "ggm", "ggplot2", "ggraph", "ggrepel", "ggridges", "ggstance", "glue", "gplots", "graphlayouts", "gtools", "Hmisc", "hms", "huge", "igraph", "insight", "isoband", "jsonlite", "knitr", "latticeExtra", "lifecycle", "lme4", "lmerTest", "loo", "lubridate", "matrixStats", "mcmc", "MCMCpack", "mime", "mnormt", "ModelMetrics", "modelr", "mosaic", "MuMIn", "mvtnorm", "nFactors", "nloptr", "nonnest2", "parameters", "pbkrtest", "performance", "pillar", "plyr", "prettyunits", "pROC", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantmod", "quantreg", "raster", "Rcpp", "RcppArmadillo", "RcppParallel", "reshape2", "RJSONIO", "rlang", "rstan", "rstanarm", "rstudioapi", "shiny", "shinyjs", "sp", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tibble", "tidyr", "tidyselect", "tinytex", "TSP", "TTR", "vctrs", "xfun", "xml2", "xts", "yaml", "zoo"))
install.packages(c("backports", "bayestestR", "BH", "blavaan", "broom", "callr", "caret", "caTools", "checkmate", "cli", "countrycode", "crosstalk", "dbplyr", "dendextend", "DescTools", "deSolve", "digest", "doBy", "dplyr", "DT", "effectsize", "emmeans", "fansi", "farver", "fma", "forcats", "foreach", "forecast", "fracdiff", "fs", "fst", "future", "future.apply", "GGally", "ggformula", "ggm", "ggplot2", "ggraph", "ggrepel", "ggridges", "ggstance", "glue", "gplots", "graphlayouts", "gtools", "Hmisc", "hms", "huge", "igraph", "insight", "isoband", "jsonlite", "knitr", "latticeExtra", "lifecycle", "lme4", "lmerTest", "loo", "lubridate", "matrixStats", "mcmc", "MCMCpack", "mime", "mnormt", "ModelMetrics", "modelr", "mosaic", "MuMIn", "mvtnorm", "nFactors", "nloptr", "nonnest2", "parameters", "pbkrtest", "performance", "pillar", "plyr", "prettyunits", "pROC", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantmod", "quantreg", "raster", "Rcpp", "RcppArmadillo", "RcppParallel", "reshape2", "RJSONIO", "rlang", "rstan", "rstanarm", "rstudioapi", "shiny", "shinyjs", "sp", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tibble", "tidyr", "tidyselect", "tinytex", "TSP", "TTR", "vctrs", "xfun", "xml2", "xts", "yaml", "zoo"))
install.packages(c("backports", "bayestestR", "BH", "blavaan", "broom", "callr", "caret", "caTools", "checkmate", "cli", "countrycode", "crosstalk", "dbplyr", "dendextend", "DescTools", "deSolve", "digest", "doBy", "dplyr", "DT", "effectsize", "emmeans", "fansi", "farver", "fma", "forcats", "foreach", "forecast", "fracdiff", "fs", "fst", "future", "future.apply", "GGally", "ggformula", "ggm", "ggplot2", "ggraph", "ggrepel", "ggridges", "ggstance", "glue", "gplots", "graphlayouts", "gtools", "Hmisc", "hms", "huge", "igraph", "insight", "isoband", "jsonlite", "knitr", "latticeExtra", "lifecycle", "lme4", "lmerTest", "loo", "lubridate", "matrixStats", "mcmc", "MCMCpack", "mime", "mnormt", "ModelMetrics", "modelr", "mosaic", "MuMIn", "mvtnorm", "nFactors", "nloptr", "nonnest2", "parameters", "pbkrtest", "performance", "pillar", "plyr", "prettyunits", "pROC", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantmod", "quantreg", "raster", "Rcpp", "RcppArmadillo", "RcppParallel", "reshape2", "RJSONIO", "rlang", "rstan", "rstanarm", "rstudioapi", "shiny", "shinyjs", "sp", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tibble", "tidyr", "tidyselect", "tinytex", "TSP", "TTR", "vctrs", "xfun", "xml2", "xts", "yaml", "zoo"))
install.packages(c("backports", "bayestestR", "BH", "blavaan", "broom", "callr", "caret", "caTools", "checkmate", "cli", "countrycode", "crosstalk", "dbplyr", "dendextend", "DescTools", "deSolve", "digest", "doBy", "dplyr", "DT", "effectsize", "emmeans", "fansi", "farver", "fma", "forcats", "foreach", "forecast", "fracdiff", "fs", "fst", "future", "future.apply", "GGally", "ggformula", "ggm", "ggplot2", "ggraph", "ggrepel", "ggridges", "ggstance", "glue", "gplots", "graphlayouts", "gtools", "Hmisc", "hms", "huge", "igraph", "insight", "isoband", "jsonlite", "knitr", "latticeExtra", "lifecycle", "lme4", "lmerTest", "loo", "lubridate", "matrixStats", "mcmc", "MCMCpack", "mime", "mnormt", "ModelMetrics", "modelr", "mosaic", "MuMIn", "mvtnorm", "nFactors", "nloptr", "nonnest2", "parameters", "pbkrtest", "performance", "pillar", "plyr", "prettyunits", "pROC", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantmod", "quantreg", "raster", "Rcpp", "RcppArmadillo", "RcppParallel", "reshape2", "RJSONIO", "rlang", "rstan", "rstanarm", "rstudioapi", "shiny", "shinyjs", "sp", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tibble", "tidyr", "tidyselect", "tinytex", "TSP", "TTR", "vctrs", "xfun", "xml2", "xts", "yaml", "zoo"))
install.packages("resample")
install.packages("glamnet")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
library(ggplot2)  # plotting
install.packages("ggplot2")
install.packages("DAAG")
install.packages("MASS")
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
# import data and examine it
greenbuildings <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q1/greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvr$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
# import data and examine it
greenbuildings <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q1/greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvr$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvl$lambda.1se  # lambda for this MSE
## [1] 0.01516562
#fitted coefficients at minimum MSE
coef(cvl, select="min")
# Apply CV Ridge regression to data
cvr <- cv.glmnet(
x ,
y ,
alpha = 0
)
# plot MSE as a function of log(lambda)
plot(cvr)
min(cvr$cvm)       # minimum MSE
## [1] 0.06679016  #value observed
cvr$lambda.min     # lambda for this min MSE
## [1] 0.03585894
cvr$cvm[cvr$lambda == cvr$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvr$lambda.1se  # lambda for this MSE
## [1] 0.0828388
#fitted coefficients at minimum MSE
coef(cvr, select="min")
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
library(ggplot2)
library(foreach)
library(LICORS)
library(caret)
library(e1071)
wine <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q3/wine.csv")
str(wine)
wine_chem_prop <- wine[1:11]
wine_chem_prop_norm = scale(wine_chem_prop, center = TRUE, scale = TRUE)
pc_chem = prcomp(wine_chem_prop_norm)
pc_chem$rotation[,1]
qplot(pc_chem$x[,1], pc_chem$x[,2],
color=wine$color, xlab='Component 1',
ylab='Component 2',geom = c("point", "abline"),intercept = 2.5, slope = 2)
y_hat2 = pc_chem$x[,2] - 2*pc_chem$x[,1] - 2.5
y_hat2 = ifelse(y_hat2 > 0, "red", "white")
confusionMatrix(data = as.factor(y_hat2), reference = wine$color)
k_grid = seq(2,20, by = 1) # This is a vector containing various values of k.
SSE_grid = foreach(k = k_grid, .combine = 'c') %do% {
cluster_k = kmeans(wine_chem_prop_norm, k, nstart = 50)
cluster_k$tot.withinss
}
plot(k_grid, SSE_grid)
clustk10 = kmeanspp(wine_chem_prop_norm, 10, nstart = 25)
cor(wine_chem_prop, as.numeric(wine$quality))
pairs(wine[, c(2,5,8,11)],
col = c("dark red", "red","pink", "grey", "blue", "dark blue", "purple")[wine$quality],
main = "Chemical Properties of Wine: Quality Scores",
oma=c(5,5,5,15))
par(xpd = TRUE)
legend("right",
fill = c("dark red", "red","pink", "grey", "blue", "dark blue", "purple"),
legend = c( levels(as.factor(wine$quality))),
title = "Score")
wine_qual = wine
wine_qual$quality = ifelse(wine$quality < 7, "low", "high")
pairs(wine_qual[, c(2,5,8,11)],
col = c("red","grey")[as.factor(wine_qual$quality)],
main = "Chemical properties of Wine: High vs Low Quality")
legend("right",
fill = c("red","grey"),
legend = c( levels(wine$color)))
pairs(wine_qual[, c(2,5,8,11)],
col = c("grey","red")[clustk2$cluster],
main = "Chemical properties of Wine: Cluster 1 vs Cluster 2")
clustk2 = kmeanspp(wine_chem_prop_norm, 2, nstart = 25)
pairs(wine_qual[, c(2,5,8,11)],
col = c("grey","red")[clustk2$cluster],
main = "Chemical properties of Wine: Cluster 1 vs Cluster 2")
pairs(wine_qual[, c(2,5,8,11)],
col = c("grey","red")[clustk2$cluster],
main = "Chemical properties of Wine: Cluster 1 vs Cluster 2")
y_hat3 = clustk2$cluster
library(ggplot2)
library(foreach)
library(LICORS)
library(caret)
library(e1071)
wine <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q3/wine.csv")
str(wine)
wine_chem_prop <- wine[1:11]
wine_chem_prop_norm = scale(wine_chem_prop, center = TRUE, scale = TRUE)
pc_chem = prcomp(wine_chem_prop_norm)
pc_chem$rotation[,1]
qplot(pc_chem$x[,1], pc_chem$x[,2],
color=wine$color, xlab='Component 1',
ylab='Component 2',geom = c("point", "abline"),intercept = 2.5, slope = 2)
y_hat2 = pc_chem$x[,2] - 2*pc_chem$x[,1] - 2.5
y_hat2 = ifelse(y_hat2 > 0, "red", "white")
confusionMatrix(data = as.factor(y_hat2), reference = wine$color)
k_grid = seq(2,20, by = 1) # This is a vector containing various values of k.
SSE_grid = foreach(k = k_grid, .combine = 'c') %do% {
cluster_k = kmeans(wine_chem_prop_norm, k, nstart = 50)
cluster_k$tot.withinss
}
plot(k_grid, SSE_grid)
clustk10 = kmeanspp(wine_chem_prop_norm, 10, nstart = 25)
cor(wine_chem_prop, as.numeric(wine$quality))
pairs(wine[, c(2,5,8,11)],
col = c("dark red", "red","pink", "grey", "blue", "dark blue", "purple")[wine$quality],
main = "Chemical Properties of Wine: Quality Scores",
oma=c(5,5,5,15))
par(xpd = TRUE)
legend("right",
fill = c("dark red", "red","pink", "grey", "blue", "dark blue", "purple"),
legend = c( levels(as.factor(wine$quality))),
title = "Score")
wine_qual = wine
wine_qual$quality = ifelse(wine$quality < 7, "low", "high")
pairs(wine_qual[, c(2,5,8,11)],
col = c("red","grey")[as.factor(wine_qual$quality)],
main = "Chemical properties of Wine: High vs Low Quality")
legend("right",
fill = c("red","grey"),
legend = c( levels(wine$color)))
pairs(wine_qual[, c(2,5,8,11)],
col = c("grey","red")[clustk2$cluster],
main = "Chemical properties of Wine: Cluster 1 vs Cluster 2")
pairs(wine_qual[, c(2,5,8,11)],
col = c("grey","red")[cluster$clustk2],
main = "Chemical properties of Wine: Cluster 1 vs Cluster 2")
clustk2 = kmeanspp(wine_chem_prop_norm, 2, nstart = 25)
library(ggplot2)
library(foreach)
library(LICORS)
library(caret)
library(e1071)
wine <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q3/wine.csv")
str(wine)
wine_chem_prop <- wine[1:11]
wine_chem_prop_norm = scale(wine_chem_prop, center = TRUE, scale = TRUE)
clustk2 = kmeanspp(wine_chem_prop_norm, 2, nstart = 25)
which(clustk2$cluster ==1)
which(clustk2$cluster ==2)
sum(clustk2$withinss)
cor(wine_chem_prop, as.numeric(wine$color))
pairs(wine[, c(2,5,7,10)], col = c("red", "grey")[wine$color],
main = "Chemical Properties of wine: Red vs White")
pairs(wine[, c(2,5,7,10)], col = c("red", "grey")[clustk2$cluster],
main = "Chemical Properties of wine: Cluster 1 vs Cluster 2")
yhat1 = clustk2$cluster
yhat1 = ifelse(yhat1 ==1, "red", "white")
confusionMatrix(data = as.factor(yhat1), reference = wine$color)
confusionMatrix(data = as.factor(rep("white", length(clustk2$cluster))), reference = wine$color)
pc_chem = prcomp(wine_chem_prop_norm)
pc_chem$rotation[,1]
qplot(pc_chem$x[,1], pc_chem$x[,2],
color=wine$color, xlab='Component 1',
ylab='Component 2',geom = c("point", "abline"),intercept = 2.5, slope = 2)
y_hat2 = pc_chem$x[,2] - 2*pc_chem$x[,1] - 2.5
y_hat2 = ifelse(y_hat2 > 0, "red", "white")
confusionMatrix(data = as.factor(y_hat2), reference = wine$color)
k_grid = seq(2,20, by = 1) # This is a vector containing various values of k.
SSE_grid = foreach(k = k_grid, .combine = 'c') %do% {
cluster_k = kmeans(wine_chem_prop_norm, k, nstart = 50)
cluster_k$tot.withinss
}
plot(k_grid, SSE_grid)
clustk10 = kmeanspp(wine_chem_prop_norm, 10, nstart = 25)
cor(wine_chem_prop, as.numeric(wine$quality))
pairs(wine[, c(2,5,8,11)],
col = c("dark red", "red","pink", "grey", "blue", "dark blue", "purple")[wine$quality],
main = "Chemical Properties of Wine: Quality Scores",
oma=c(5,5,5,15))
par(xpd = TRUE)
legend("right",
fill = c("dark red", "red","pink", "grey", "blue", "dark blue", "purple"),
legend = c( levels(as.factor(wine$quality))),
title = "Score")
wine_qual = wine
wine_qual$quality = ifelse(wine$quality < 7, "low", "high")
pairs(wine_qual[, c(2,5,8,11)],
col = c("red","grey")[as.factor(wine_qual$quality)],
main = "Chemical properties of Wine: High vs Low Quality")
legend("right",
fill = c("red","grey"),
legend = c( levels(wine$color)))
pairs(wine_qual[, c(2,5,8,11)],
col = c("grey","red")[clustk2$cluster],
main = "Chemical properties of Wine: Cluster 1 vs Cluster 2")
y_hat3 = clustk2$cluster
y_hat3 = ifelse(clustk2$cluster == 1, "low", "high")
confusionMatrix(data = as.factor(y_hat3), reference = as.factor(wine_qual$quality))
library(ggplot2)
library(foreach)
library(LICORS)
library(caret)
library(e1071)
wine <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q3/wine.csv")
str(wine)
wine_chem_prop <- wine[1:11]
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
# import data and examine it
greenbuildings <- read.csv("greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvl$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvl$lambda.1se  # lambda for this MSE
## [1] 0.01516562
#fitted coefficients at minimum MSE
coef(cvl, select="min")
# Apply CV Ridge regression to data
cvr <- cv.glmnet(
x ,
y ,
alpha = 0
)
# plot MSE as a function of log(lambda)
plot(cvr)
min(cvr$cvm)       # minimum MSE
## [1] 0.06679016  #value observed
cvr$lambda.min     # lambda for this min MSE
## [1] 0.03585894
cvr$cvm[cvr$lambda == cvr$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvr$lambda.1se  # lambda for this MSE
## [1] 0.0828388
#fitted coefficients at minimum MSE
coef(cvr, select="min")
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = TRUE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
summary(linear_fit)
summary(cvl)
summary(linear_fit)
summary(Cvr)
